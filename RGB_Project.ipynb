{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nl9EMktBLFXW",
        "outputId": "ec9066d8-32fc-4bb5-85db-56990f99d46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m900.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.17.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.1 (from gradio)\n",
            "  Downloading gradio_client-1.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.1->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.1->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.17.1-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.1-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.17.1 gradio-client-1.7.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ],
      "source": [
        "#Install and upgrade the necessary libraries for Hugging Face models, LangChain, OpenAI, and Gradio.\n",
        "\n",
        "!pip -q install --upgrade huggingface_hub\n",
        "!pip -q install langchain_community\n",
        "!pip -q install langchain_huggingface\n",
        "!pip install huggingface_hub\n",
        "!pip install openai\n",
        "!pip install gradio\n",
        "!pip install groq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y7kZHpXUl0rq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "#Allows interaction with the operating system.\n",
        "import os\n",
        "#Provides access to system-specific parameters and functions.\n",
        "import sys\n",
        "#Imports the OpenAI Python library for interacting with OpenAI models like GPT-4.\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "#Provides support for reading configuration files (.ini).\n",
        "import configparser\n",
        "#Imports Hugging Face Endpoint support from LangChain.\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "#Imports GenerationConfig, which is used to configure text generation parameters.\n",
        "from transformers import GenerationConfig\n",
        "#A popular library for making HTTP requests.\n",
        "import requests\n",
        "#Imports TQDM, a library for progress bars.\n",
        "import tqdm\n",
        "#Enables Google Drive integration in Google Colab\n",
        "from google.colab import drive\n",
        "#Provides pre-trained tokenizer and model utilities for text generation.\n",
        "#AutoTokenizer: Loads the appropriate tokenizer for a given model.\n",
        "#AutoModel: Loads a general-purpose model.\n",
        "#AutoModelForCausalLM: Loads a causal language model (LLM) for text generation.\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import random, math\n",
        "import gradio as gr\n",
        "import groq\n",
        "from groq import Groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "# 'force_remount=True' ensures that the drive is unmounted and remounted if already mounted\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "\n",
        "# Define the base folder path where configuration files are stored in Google Drive\n",
        "# Ensure that the folder \"RGB_Data_Config\" exists in your Google Drive\n",
        "RGB_Config_folder_path = \"/content/drive/My Drive/RGB_Data_Config/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GkQkYlpdg73",
        "outputId": "82bf6857-e456-4f9b-ac3e-50a57d4e7fb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_config(config_file_path):\n",
        "    # Check if the configuration file exists at the given path\n",
        "    if not os.path.exists(config_file_path):\n",
        "        # Raise an error if the file doesn't exist\n",
        "        raise FileNotFoundError(f\"Configuration file not found: {config_file_path}\")\n",
        "\n",
        "    # Initialize a ConfigParser object to read and handle the INI configuration file\n",
        "    config = configparser.ConfigParser()\n",
        "\n",
        "    try:\n",
        "        # Attempt to read the configuration file\n",
        "        config.read(config_file_path)\n",
        "    except Exception as e:\n",
        "        # If reading the file fails, raise an error with details\n",
        "        raise ValueError(f\"Failed to read configuration file: {config_file_path}. Error: {e}\")\n",
        "\n",
        "    # Return the parsed configuration object\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "koANUqm94SnL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "drdz05K21t2g"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "# Define the path to the debug file. This file will be used for logging debug information.\n",
        "debug_file_name = RGB_Config_folder_path + \"debug.txt\"\n",
        "\n",
        "# Read the configuration from a specified INI file located in the folder defined by RGB_Config_folder_path.\n",
        "# The configuration is stored in the `Config` object for later use.\n",
        "Config = read_config(RGB_Config_folder_path + 'config.ini')\n",
        "\n",
        "# Extract the Hugging Face API key from the 'Settings' section of the configuration file.\n",
        "hfapi_key = Config.get('Settings', 'hfapi_key')\n",
        "\n",
        "# Extract the Groq API key from the 'Settings' section of the configuration file.\n",
        "#groq_key = \"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\" # Config.get('Settings', 'groq_key')\n",
        "groq_key = \"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\"\n",
        "\n",
        "# Extract the OpenAI API key from the 'Settings' section of the configuration file.\n",
        "openai_api_key = Config.get('Settings', 'openai_api_key')\n",
        "\n",
        "# Define the URL for OpenAI's chat completions API.\n",
        "open_api_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "# Define the number of questions to be used or processed.\n",
        "# This could represent a batch size or a limit in certain operations.\n",
        "Number_of_questions = 300\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Embedding LLM configuration\n",
        "from huggingface_hub import login\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "class HuggingFaceLLM:\n",
        "    # Initialization of the class with model and optional URL\n",
        "    def __init__(self, model, url=None):\n",
        "        self.model = model  # Set the model name\n",
        "        self.set_hf_login()  # Set up Hugging Face authentication\n",
        "        # Initialize the InferenceClient with the model\n",
        "        self.client = InferenceClient(self.model)\n",
        "\n",
        "        # Create HuggingFace LLM endpoint with specific configurations for text generation\n",
        "        self.LLM1 = HuggingFaceEndpoint(\n",
        "            repo_id=model,  # Model repo\n",
        "            task=\"text-generation\",  # Task type is text generation\n",
        "            max_new_tokens=1024,  # Limit on new tokens generated\n",
        "            temperature=0.1,  # Controls randomness of generation\n",
        "            top_k=30,  # Limits sampling to top 30 tokens\n",
        "            repetition_penalty=1.03  # Penalty for repeating text\n",
        "        )\n",
        "\n",
        "    # Method to set Hugging Face login by managing environment variables\n",
        "    def set_hf_login(self):\n",
        "        if \"HF_TOKEN\" in os.environ:\n",
        "            del os.environ[\"HF_TOKEN\"]  # Remove existing token if present\n",
        "        if \"HUGGINGFACEHUB_API_TOKEN\" in os.environ:\n",
        "            del os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]  # Same for the second token\n",
        "        os.environ[\"HF_TOKEN\"] = hfapi_key  # Set new token\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hfapi_key  # Set token for Hugging Face Hub\n",
        "\n",
        "    # Method to generate a response from the Hugging Face model\n",
        "    def generate(self, text, system=\"\", temperature=0.7, top_p=1.0):\n",
        "        headers = {\"Authorization\": f\"Bearer {hfapi_key}\"}  # Bearer token for authentication\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": f\"{system}\\n{text}\",  # Combine system instruction and user input\n",
        "            \"parameters\": {\n",
        "                \"temperature\": temperature,  # Sampling temperature\n",
        "                \"top_p\": top_p,  # Nucleus sampling\n",
        "                \"return_full_text\": False  # Only return the generated text\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Use Hugging Face model endpoint to generate the text based on the provided inputs\n",
        "        response = self.LLM1.invoke(f\"{system}\\n{text}\")\n",
        "        print(response)  # Print the generated response for debugging purposes\n",
        "\n",
        "        return response  # Return the generated response\n",
        "\n",
        "\n",
        "class GroqModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.client = groq.Groq(api_key=groq_key)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def generate(self, prompt, temperature, system=None):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[{\"role\": \"system\", \"content\": system or \"\"},\n",
        "                      {\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# OpenAI LLM configuration\n",
        "class OpenAILLM:\n",
        "    # Initialization with model and max token limit for OpenAI LLM\n",
        "    def __init__(self, QA_Model, max_tokens):\n",
        "        openai.api_key = openai_api_key  # Set OpenAI API key\n",
        "        self.API_KEY = openai_api_key  # Store the key for API calls\n",
        "        self.Open_AI_LLM_inst = OpenAI(api_key=openai_api_key)  # Create an OpenAI instance\n",
        "        self.model = QA_Model  # Model name\n",
        "        self.max_tokens = max_tokens  # Maximum token limit for OpenAI API response\n",
        "        self.url = open_api_url  # OpenAI API URL\n",
        "\n",
        "    # Method to generate a response from the OpenAI model\n",
        "    def generate(self, text: str, temperature=0.7, system=\"You are a helpful assistant. You can help me by answering my questions. You can also ask me questions.\", top_p=1):\n",
        "        headers = {\"Authorization\": f\"Bearer {self.API_KEY}\"}  # Bearer token for OpenAI API\n",
        "\n",
        "        query = {\n",
        "            \"model\": self.model,  # Model to use for text generation\n",
        "            \"temperature\": temperature,  # Sampling temperature for randomness\n",
        "            \"top_p\": top_p,  # Nucleus sampling parameter\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system},  # System message defining the role\n",
        "                {\"role\": \"user\", \"content\": text}  # User input text\n",
        "            ],\n",
        "            \"stream\": False  # Stream option to get the response in chunks (set to False for a single response)\n",
        "        }\n",
        "\n",
        "        responses = requests.post(self.url, headers=headers, json=query)  # Send the request to OpenAI API\n",
        "\n",
        "        if 'choices' not in responses.json():\n",
        "            print(text)  # Print the input if no valid response is returned\n",
        "            print(responses)  # Print the full API response for debugging\n",
        "\n",
        "        # Print and return the generated response from OpenAI\n",
        "        print(responses.json()['choices'][0]['message']['content'])\n",
        "        return responses.json()['choices'][0]['message']['content']\n",
        "\n",
        "    # Function to get a specific OpenAI response to a prompt\n",
        "    def get_openai_response(self, prompt):\n",
        "        try:\n",
        "            # Prepare the message list with prompt and test question\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": prompt},  # System instruction\n",
        "                {\"role\": \"user\", \"content\": \"test question?\"},  # User query\n",
        "            ]\n",
        "\n",
        "            # Use OpenAI LLM's beta chat completion API\n",
        "            response = self.Open_AI_LLM_inst.beta.chat.completions.parse(\n",
        "                model=self.QA_Model, messages=messages, max_tokens=self.max_tokens\n",
        "            )\n",
        "\n",
        "            # Return the content of the first choice (answer) from the OpenAI response\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any errors and return an error message\n",
        "            return f\"Error: {e}\"\n"
      ],
      "metadata": {
        "id": "QDKolFHaDnQA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the data for a given instance based on noise rate, passage number, filename, and correctness rate.\n",
        "def processdata(instance, noise_rate, passage_num, filename, correct_rate):\n",
        "    # Extract the query and answer from the instance.\n",
        "    query = instance['query']\n",
        "    ans = instance['answer']\n",
        "\n",
        "    # Calculate the number of negative and positive samples based on the noise rate.\n",
        "    neg_num = math.ceil(passage_num * noise_rate)\n",
        "    pos_num = passage_num - neg_num\n",
        "\n",
        "    # Case when the filename contains '_int' (could indicate some internal data or instruction-based format)\n",
        "    if '_int' in filename:\n",
        "        # Shuffle the positive samples\n",
        "        for i in instance['positive']:\n",
        "            random.shuffle(i)\n",
        "        print(len(instance['positive']))  # Debugging output: print the length of positive samples\n",
        "        # Select the first document from each positive set\n",
        "        docs = [i[0] for i in instance['positive']]\n",
        "\n",
        "        # If there are not enough positive documents, fill in with additional documents from larger positive sets\n",
        "        if len(docs) < pos_num:\n",
        "            maxnum = max([len(i) for i in instance['positive']])\n",
        "            for i in range(1, maxnum):\n",
        "                for j in instance['positive']:\n",
        "                    if len(j) > i:\n",
        "                        docs.append(j[i])\n",
        "                        if len(docs) == pos_num:\n",
        "                            break\n",
        "                if len(docs) == pos_num:\n",
        "                    break\n",
        "        # Calculate the number of negative documents needed\n",
        "        neg_num = passage_num - len(docs)\n",
        "        # Add negative documents if needed\n",
        "        if neg_num > 0:\n",
        "            negative = instance['negative'][:neg_num]\n",
        "            docs += negative\n",
        "    # Case when the filename contains '_fact' (likely for factual questions or true/false type tasks)\n",
        "    elif '_fact' in filename:\n",
        "        # Calculate the number of correct documents to select based on the correct rate.\n",
        "        correct_num = math.ceil(passage_num * correct_rate)\n",
        "        pos_num = passage_num - neg_num - correct_num\n",
        "        # Randomly select positive examples\n",
        "        indexs = list(range(len(instance['positive'])))\n",
        "        selected = random.sample(indexs, min(len(indexs), pos_num))\n",
        "        docs = [instance['positive_wrong'][i] for i in selected]\n",
        "\n",
        "        # Remaining indices after selecting positive examples\n",
        "        remain = [i for i in indexs if i not in selected]\n",
        "        # Add correct examples if available\n",
        "        if correct_num > 0 and len(remain) > 0:\n",
        "            docs += [instance['positive'][i] for i in random.sample(remain, min(len(remain), correct_num))]\n",
        "\n",
        "        # Add negative examples if needed\n",
        "        if neg_num > 0:\n",
        "            docs += instance['negative'][:neg_num]\n",
        "    else:\n",
        "        # General case for other types of data (no '_int' or '_fact' in filename)\n",
        "        if noise_rate == 1:\n",
        "            neg_num = passage_num  # All passages are negative if noise_rate is 1\n",
        "            pos_num = 0\n",
        "        else:\n",
        "            # Adjust the number of positive and negative passages based on the available data\n",
        "            if neg_num > len(instance['negative']):\n",
        "                neg_num = len(instance['negative'])\n",
        "                pos_num = passage_num - neg_num\n",
        "            elif pos_num > len(instance['positive']):\n",
        "                pos_num = len(instance['positive'])\n",
        "                neg_num = passage_num - pos_num\n",
        "\n",
        "        # Select the required number of positive and negative samples\n",
        "        positive = instance['positive'][:pos_num]\n",
        "        negative = instance['negative'][:neg_num]\n",
        "\n",
        "        # Combine the selected positive and negative samples\n",
        "        docs = positive + negative\n",
        "\n",
        "    # Shuffle the final document list\n",
        "    random.shuffle(docs)\n",
        "\n",
        "    return query, ans, docs  # Return the query, answer, and the processed documents\n",
        "\n",
        "\n",
        "# Function to check if the predicted answer matches the ground truth.\n",
        "def checkanswer(prediction, ground_truth):\n",
        "    prediction = prediction.lower()  # Convert prediction to lowercase for case-insensitive comparison\n",
        "    if type(ground_truth) is not list:\n",
        "        ground_truth = [ground_truth]  # Ensure ground_truth is always a list\n",
        "\n",
        "    labels = []\n",
        "    for instance in ground_truth:\n",
        "        flag = True\n",
        "        if type(instance) == list:  # If the ground truth is a list of possible answers\n",
        "            flag = False\n",
        "            instance = [i.lower() for i in instance]  # Lowercase all possible answers\n",
        "            for i in instance:\n",
        "                if i in prediction:\n",
        "                    flag = True  # Set flag to True if any possible answer is found in prediction\n",
        "                    break\n",
        "        else:\n",
        "            instance = instance.lower()  # Lowercase the single ground truth answer\n",
        "            if instance not in prediction:\n",
        "                flag = False  # Set flag to False if the ground truth is not found in prediction\n",
        "        labels.append(int(flag))  # Convert boolean flag to integer (1 for correct, 0 for incorrect)\n",
        "\n",
        "    return labels  # Return the list of labels (1 for correct, 0 for incorrect)\n",
        "\n",
        "\n",
        "# Function to evaluate the results, checking for correctness based on a threshold.\n",
        "def getevalue(results):\n",
        "    results = np.array(results)  # Convert results to a numpy array for easier manipulation\n",
        "    results = np.max(results, axis=0)  # Take the max along axis 0 (likely across multiple predictions)\n",
        "    if 0 in results:\n",
        "        return False  # If any result is 0 (incorrect), return False\n",
        "    else:\n",
        "        return True  # If all results are correct (1), return True\n",
        "\n",
        "\n",
        "# Main prediction function that interacts with the model and generates the predictions.\n",
        "def predict(query, ground_truth, docs, model, system, instruction, temperature, dataset):\n",
        "    '''\n",
        "    label: 0 for positive, 1 for negative, -1 for not enough information\n",
        "    '''\n",
        "\n",
        "    # If no documents are provided, generate a prediction with an empty document list.\n",
        "    if len(docs) == 0:\n",
        "        text = instruction.format(QUERY=query, DOCS='')  # Format instruction with query and empty docs\n",
        "        prediction = model.generate(text, temperature)  # Get prediction from the model\n",
        "    else:\n",
        "        # Format instruction with query and the list of documents\n",
        "        docs = '\\n'.join(docs)\n",
        "        text = instruction.format(QUERY=query, DOCS=docs)\n",
        "        prediction = model.generate(text, temperature, system)  # Get prediction with system instructions\n",
        "\n",
        "    # Handle dataset-specific processing, e.g., removing spaces for Chinese datasets\n",
        "    if 'zh' in dataset:\n",
        "        prediction = prediction.replace(\" \", \"\")  # Remove spaces if the dataset is in Chinese\n",
        "\n",
        "    # If the prediction indicates insufficient information, label as -1 (not enough info)\n",
        "    if '信息不足' in prediction or 'insufficient information' in prediction:\n",
        "        labels = [-1]\n",
        "    else:\n",
        "        # Check if the prediction matches the ground truth and return the label (0 for correct, 1 for incorrect)\n",
        "        labels = checkanswer(prediction, ground_truth)\n",
        "\n",
        "    # Initialize a fact label (for factual correctness checking)\n",
        "    factlabel = 0\n",
        "    # If the prediction mentions factual errors, set factlabel to 1 (indicating factual error)\n",
        "    if '事实性错误' in prediction or 'factual errors' in prediction:\n",
        "        factlabel = 1\n",
        "\n",
        "    return labels, prediction, factlabel  # Return the labels, prediction, and factual correctness label\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ef0_3KHxg3yR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ly0T8Gh_GWFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluating ChatGPT, you can run as:\n",
        "\n",
        "python evalue.py \\\n",
        "--dataset en \\\n",
        "--modelname chatgpt \\\n",
        "--temp 0.2 \\\n",
        "--noise_rate 0.6 \\\n",
        "--api_key YourAPIKEY \\\n",
        "--passage_num 5\n",
        "For evaluating other models, you can run as:\n",
        "\n",
        "python evalue.py \\\n",
        "--dataset en \\\n",
        "--modelname chatglm2-6b \\\n",
        "--temp 0.2 \\\n",
        "--noise_rate 0.6 \\\n",
        "--plm THUDM/chatglm-6b \\\n",
        "--passage_num 5\n",
        "You should change modelname and plm for different models, where plm is the path of model.\n",
        "\n",
        "temp is the temperature of model.\n",
        "\n",
        "noise_rate is rate of noisy documents in inputs.\n",
        "\n",
        "passage_num is number of provided documents for LLM (default is 5).\n",
        "\n",
        "The outputs are:\n",
        "\n",
        "all_rate: The accuracy (noise_rate<1) or rejection rate (noise_rate=1)\n",
        "fact_check_rate: the error detection rates (ED)\n",
        "To evaluate rejection using ChatGPT, you should first run the evalue.py in noise_rate=1 to obtain the generation result, and then run:\n",
        "\n",
        "python reject_evalue.py \\\n",
        "--dataset en \\\n",
        "--modelname chatglm2-6b \\\n",
        "--api_key YourAPIKEY\n",
        "The \"reject_rate\" in the outputs are the reject rate (Rej*).\n",
        "\n",
        "To evaluate counterfactual robustness using ChatGPT, you should first run the evalue.py in dataset=en_fact/zh_fact to obtain the generation result, and then run:\n",
        "\n",
        "python fact_evalue.py \\\n",
        "--dataset en_fact \\\n",
        "--modelname chatglm2-6b \\\n",
        "--api_key YourAPIKEY\n",
        "The \"reject_rate\" in the outputs are the error detection rates (ED*). The correct_rate in the outputs are the error correction rate (CR)"
      ],
      "metadata": {
        "id": "EiGixufxC4v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to determine if a model can identify factual errors in its response\n",
        "def fact_check(question, answer, url, apikey):\n",
        "    # Define a prompt to guide the model to evaluate whether factual errors exist\n",
        "    prompt = '''Please determine whether the model can identify factual errors in the document. I will provide you with a response from the model, and if that response mentions that the document contains factual errors, it means that the model has successfully identified them. Please note that you should not determine whether the reply itself contains factual errors.\n",
        "Demonstrations:\n",
        "Response:文档信息有误，2008年的奥运会在北京举办。\n",
        "Yes, the model has identified the factual errors.\n",
        "\n",
        "Response:2014年澳网女单冠军是大阪直美。\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response: The director of the Silent Hill movie is Justin Kurzel.\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response: Harry Potter is written by J. K. Rowling.\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response:  There are factual errors in the provided documents. The correct answer is 2023.\n",
        "Yes, the model has identified the factual errors.\n",
        "\n",
        "Begin to generate:\n",
        "Answer: {answer}\n",
        "    '''\n",
        "    # Format the prompt with the provided answer to evaluate its factual correctness\n",
        "    text2 = prompt.format(answer=answer)\n",
        "\n",
        "    # Call the fact_getdata function to send the prompt to the model and return its response\n",
        "    return fact_getdata(text2, url, apikey)\n",
        "\n",
        "\n",
        "# Function to send the prompt to the API and retrieve the model's evaluation\n",
        "def fact_getdata(text, url, API_KEY):\n",
        "    # Prepare the data to send to the API, including the model's input (prompt)\n",
        "    data = {\n",
        "        \"model\": \"gpt-3.5-turbo\",  # Specify the model being used\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": text}]  # Format the prompt for the model\n",
        "    }\n",
        "\n",
        "    # Set the authorization header with the API key\n",
        "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
        "\n",
        "    # Send a POST request to the API with the data and headers\n",
        "    completion = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "    # Parse the response from the API and return the model's evaluation\n",
        "    completion = completion.json()['choices'][0]['message']['content']\n",
        "    return completion\n",
        "\n",
        "\n",
        "# Main function for evaluating the fact-checking ability of the model on a dataset\n",
        "def fact_evalue(modelname, dataset_file_name, temperature, noise_rate, correct_rate, passage_num):\n",
        "    # Define the paths for saving the results\n",
        "    resultpath = 'result-en/fact'\n",
        "    normal_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json'\n",
        "    chatgptresult_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgptresult.json'\n",
        "    chatgpt_outputfile = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgpt.json'\n",
        "\n",
        "    results = []  # List to store the results\n",
        "    useddata = {}  # Dictionary to store previously processed data (to avoid redundant processing)\n",
        "\n",
        "    # If results have already been saved in a previous run, load them\n",
        "    if os.path.exists(chatgpt_outputfile):\n",
        "        with open(chatgpt_outputfile) as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data  # Store the data by ID\n",
        "\n",
        "    # Open the output file to save the new evaluation results\n",
        "    with open(chatgpt_outputfile, 'w', encoding='utf-8') as f:\n",
        "        with open(normal_dump_file, 'r', encoding='utf-8') as f2:\n",
        "            for line in tqdm.tqdm(f2):  # Iterate through each line in the normal dump file\n",
        "                data = json.loads(line)\n",
        "\n",
        "                # If the data has already been processed, append it to the results\n",
        "                if data['id'] in useddata:\n",
        "                    results.append(useddata[data['id']])\n",
        "                    f.write(json.dumps(useddata[data['id']], ensure_ascii=False) + '\\n')\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Extract the question and answer from the data\n",
        "                    question = data['query']\n",
        "                    answer = data['prediction']\n",
        "\n",
        "                    # Call the fact_check function to evaluate the response for factual errors\n",
        "                    evaluation = fact_check(question, answer, open_api_url, openai_api_key)\n",
        "\n",
        "                    # Add the evaluation result to the data\n",
        "                    data['evaluation'] = evaluation\n",
        "                    results.append(data)\n",
        "\n",
        "                    # Write the updated data (with evaluation) to the output file\n",
        "                    f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                except Exception as e:\n",
        "                    # In case of an error, print the error and skip this data point\n",
        "                    print(e)\n",
        "                    print(question, answer)\n",
        "                    continue\n",
        "\n",
        "    # Initialize counters for various statistics\n",
        "    rejecttt = 0  # Count of responses where factual errors are identified\n",
        "    tt = 0  # Count of responses where the model made a correct prediction\n",
        "    correct_tt = 0  # Count of correct responses that were flagged as having factual errors\n",
        "\n",
        "    # Loop through the results and compute the statistics\n",
        "    for i in results:\n",
        "        if \"has identified\" in i['evaluation'] or \"Yes\" in i['evaluation']:\n",
        "            rejecttt += 1  # Increment reject count if the evaluation mentions factual errors\n",
        "            if 0 not in i['label'] and 1 in i['label']:\n",
        "                correct_tt += 1  # If the model correctly identifies the factual error, increment the correct count\n",
        "        if 0 not in i['label'] and 1 in i['label']:\n",
        "            tt += 1  # Increment total correct count if the label indicates a correct answer\n",
        "\n",
        "    # Calculate the evaluation metrics\n",
        "    scores = {\n",
        "        'reject_rate': rejecttt / len(results),  # Percentage of responses with identified factual errors\n",
        "        'all_rate': (tt) / len(results),  # Overall accuracy rate\n",
        "        'correct_rate': correct_tt / rejecttt if rejecttt > 0 else 0,  # Accuracy rate for factual error identification\n",
        "        'tt': tt,  # Total correct responses\n",
        "        'rejecttt': rejecttt,  # Total factual errors identified\n",
        "        'correct_tt': correct_tt,  # Correct responses that identified factual errors\n",
        "        'nums': len(results),  # Total number of results\n",
        "        'noise_rate': noise_rate,  # Noise rate for the evaluation\n",
        "    }\n",
        "\n",
        "    # Save the evaluation scores to a JSON file\n",
        "    json.dump(scores, open(chatgptresult_dump_file, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Return the evaluation rates\n",
        "    all_rate = (tt) / len(results)  # Overall accuracy\n",
        "    reject_rate = rejecttt / len(results)  # Rate of factual errors identified\n",
        "    correct_rate = correct_tt / rejecttt if rejecttt > 0 else 0  # Accuracy in identifying factual errors\n",
        "\n",
        "    return all_rate, reject_rate, correct_rate\n"
      ],
      "metadata": {
        "id": "OErK3f75Xg2D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This function checks whether the answer to a question is supported or addressed by the document.\n",
        "def reject_check(question, answer, url, apikey):\n",
        "    # Define the prompt structure that will ask the AI model to determine if the answer is addressed by the documents\n",
        "    prompt = '''I will give you a question and an answer generated through document retrieval. Please use this answer to determine if the retrieved document can solve the question.\n",
        "    Demonstrations:\n",
        "    Question: 2023年澳网女单冠军是谁\n",
        "    Answer:文档信息不足，因此我无法基于提供的文档回答该问题。\n",
        "    No, the question is not addressed by the documents.\n",
        "\n",
        "    Question: Who is the champion of Australian Open 2023 Women's Singles?\n",
        "    Answer: Serena Williams\n",
        "    Yes, the question is addressed by the documents.\n",
        "\n",
        "    Question: Where is ACL2023 held?\n",
        "    Answer: Location of ACL2023 has not been confirmed.\n",
        "    No, the question is not addressed by the documents.\n",
        "\n",
        "    Question: 2023年中国GDP是多少?\n",
        "    Answer: I can not answer this question。\n",
        "    No, the question is not addressed by the documents.\n",
        "\n",
        "    Begin to generate:\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    '''\n",
        "    # Format the prompt with the actual question and answer\n",
        "    text2 = prompt.format(question=question, answer=answer)\n",
        "    # Call the reject_getdata function to get the AI evaluation\n",
        "    return reject_getdata(text2, url, apikey)\n",
        "\n",
        "\n",
        "# This function sends the formatted prompt to the API and retrieves the response.\n",
        "def reject_getdata(text, url, API_KEY):\n",
        "    # Define the data structure for the POST request with the formatted prompt\n",
        "    data = {\n",
        "        \"model\": \"gpt-3.5-turbo\",  # We are using GPT-3.5 for this task\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": text}]\n",
        "    }\n",
        "    # Set up the headers with the API key for authentication\n",
        "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
        "    # Send a POST request to the external API and get the response\n",
        "    completion = requests.post(url, json=data, headers=headers)\n",
        "    # Extract the content of the response message and return it\n",
        "    completion = completion.json()['choices'][0]['message']['content']\n",
        "    return completion\n",
        "\n",
        "\n",
        "# This function evaluates the model's answers and checks if they address the questions correctly.\n",
        "def reject_evalue(modelname, dataset_file_name, temperature, noise_rate, correct_rate, passage_num):\n",
        "    resultpath = 'result-en'  # Define the directory where results will be stored\n",
        "\n",
        "    # Construct the file names for storing predictions and results\n",
        "    normal_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json'\n",
        "    chatgptresult_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgptresult.json'\n",
        "    chatgpt_outputfile = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_chatgpt.json'\n",
        "\n",
        "    results = []  # List to store all results\n",
        "    useddata = {}  # Dictionary to store previously processed data\n",
        "\n",
        "    # Check if the results already exist in the chatgpt_outputfile, if so, load them\n",
        "    if os.path.exists(chatgpt_outputfile):\n",
        "        with open(chatgpt_outputfile) as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data\n",
        "\n",
        "    # Open the file to write the results\n",
        "    with open(chatgpt_outputfile, 'w', encoding='utf-8') as f:\n",
        "        # Open the original file containing the model predictions\n",
        "        with open(normal_dump_file, 'r', encoding='utf-8') as f2:\n",
        "            for line in tqdm.tqdm(f2):  # Iterate through the predictions\n",
        "                data = json.loads(line)\n",
        "                # If the data has been processed before, write it back to the output file\n",
        "                if data['id'] in useddata and data['query'] == useddata[data['id']]['query'] and data['ans'] == useddata[data['id']]['ans']:\n",
        "                    results.append(useddata[data['id']])\n",
        "                    f.write(json.dumps(useddata[data['id']], ensure_ascii=False) + '\\n')\n",
        "                    continue\n",
        "                try:\n",
        "                    # Extract the question and predicted answer\n",
        "                    question = data['query']\n",
        "                    answer = data['prediction']\n",
        "\n",
        "                    # Call reject_check to determine if the answer addresses the question\n",
        "                    evaluation = reject_check(question, answer, open_api_url, openai_api_key)\n",
        "                    # Add the evaluation result to the data\n",
        "                    data['evaluation'] = evaluation\n",
        "                    results.append(data)\n",
        "                    # Write the processed data with the evaluation to the file\n",
        "                    f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                except Exception as e:\n",
        "                    print(e)  # Print any errors encountered during processing\n",
        "                    print(question, answer)\n",
        "                    continue\n",
        "\n",
        "    # Variables to track the results of the evaluation\n",
        "    rejecttt = 0\n",
        "    tt = 0\n",
        "\n",
        "    # Process the results to calculate metrics\n",
        "    for i in results:\n",
        "        # Count how many answers were rejected because they didn't address the question\n",
        "        if \"not addressed\" in i['evaluation']:\n",
        "            rejecttt += 1\n",
        "        # Count how many answers are considered correct based on labels\n",
        "        if 0 not in i['label'] and 1 in i['label']:\n",
        "            tt += 1\n",
        "\n",
        "    # Prepare the final scores for evaluation\n",
        "    scores = {\n",
        "        'reject_rate': rejecttt / len(results),  # Percentage of rejected answers\n",
        "        'all_rate': (tt) / len(results),  # Overall rate of correct answers\n",
        "        'tt': tt,  # Total correct answers\n",
        "        'rejecttt': rejecttt,  # Total rejected answers\n",
        "        'nums': len(results),  # Total number of evaluated samples\n",
        "    }\n",
        "\n",
        "    # Write the evaluation scores to a JSON file\n",
        "    json.dump(scores, open(chatgptresult_dump_file, 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Calculate the reject rate and overall rate\n",
        "    reject_rate = rejecttt / len(results)\n",
        "    all_rate = (tt) / len(results)\n",
        "\n",
        "    return reject_rate  # Return the reject rate as the output\n"
      ],
      "metadata": {
        "id": "PLBu85xaTo7U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalue(modelname, dataset_file_name, temperature, noise_rate, correct_rate, passage_num, factchecking=False):\n",
        "    # Initialize list to hold instances of the dataset\n",
        "    instances = []\n",
        "    q_no = 0  # Counter for the number of questions processed\n",
        "\n",
        "    # Open the dataset file and load data into the instances list\n",
        "    with open(dataset_file_name, 'r') as f:\n",
        "        for line in f:\n",
        "            q_no += 1\n",
        "            if q_no > Number_of_questions:  # Stop once we've processed the desired number of questions\n",
        "                break\n",
        "            instances.append(json.loads(line))  # Add each instance (JSON) to the instances list\n",
        "\n",
        "    # Define the directory for storing results\n",
        "    resultpath = 'result-en'\n",
        "    if not os.path.exists(resultpath):\n",
        "        os.mkdir(resultpath)  # Create the result directory if it doesn't exist\n",
        "\n",
        "    # Define the system and instruction text based on whether fact checking is enabled\n",
        "    if factchecking:\n",
        "        system = \"You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate ’I can not answer the question because of the insufficient information in documents.‘. If there are inconsistencies with the facts in some of the documents, please generate the response 'There are factual errors in the provided documents.' and provide the correct answer.\"\n",
        "        instruction = \"Document:\\n{DOCS} \\n\\nQuestion:\\n{QUERY}\"\n",
        "        resultpath = resultpath + '/fact'  # Save fact checking results in a separate folder\n",
        "    else:\n",
        "        system = \"You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate ’I can not answer the question because of the insufficient information in documents.‘. If there are inconsistencies with the facts in some of the documents, please generate the response 'There are factual errors in the provided documents.' and provide the correct answer.\"\n",
        "        instruction = \"Document:\\n{DOCS} \\n\\nQuestion:\\n{QUERY}\"\n",
        "\n",
        "    # Load the appropriate model based on the model name\n",
        "    if False:\n",
        "      if modelname == 'gpt-3.5-turbo':\n",
        "          model = OpenAILLM(\"gpt-3.5-turbo\", 1250)\n",
        "      elif modelname == 'Qwen2.5-72B-Instruct':\n",
        "          model = HuggingFaceLLM(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "      elif modelname == 'DeepSeek-R1-Distill-Llama-70B':\n",
        "          model = HuggingFaceLLM(\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
        "      elif modelname == 'Mixtral-8x7B-Instruct':\n",
        "          model = HuggingFaceLLM(\"mistralai/Mixtral-8x7B-Instruct\")\n",
        "      elif modelname == 'Meta-Llama-3.1-Instruct':\n",
        "          model = HuggingFaceLLM(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
        "      elif modelname == 'gemma-2-9b-it':\n",
        "          model = HuggingFaceLLM(\"google/gemma-2-9b-it\")\n",
        "      else:\n",
        "          print(\"Invalid model name\")  # Exit if an invalid model name is provided\n",
        "          sys.exit(1)\n",
        "    else:\n",
        "      if modelname == 'gpt-3.5-turbo':\n",
        "          model = OpenAILLM(\"gpt-3.5-turbo\", 1250)\n",
        "      elif modelname == 'qwen-2.5-32b':\n",
        "          model = GroqModel(\"qwen-2.5-32b\")\n",
        "      elif modelname == 'DeepSeek-R1-Distill-Llama-70B':\n",
        "          model = GroqModel(\"deepseek-r1-distill-llama-70b\")\n",
        "      elif modelname == 'Mixtral-8x7B-Instruct':\n",
        "          model = GroqModel(\"mixtral-8x7b-32768\")\n",
        "      elif modelname == 'llama3-70b-8192':\n",
        "          #model = GroqModel(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
        "          model = GroqModel(\"llama3-70b-8192\")\n",
        "      elif modelname == 'gemma-2-9b-it':\n",
        "          model = GroqModel(\"gemma2-9b-it\")\n",
        "      else:\n",
        "          print(\"Invalid model name\")  # Exit if an invalid model name is provided\n",
        "          sys.exit(1)\n",
        "\n",
        "    # Define the result file paths where the predictions will be saved\n",
        "    normal_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json'\n",
        "    result_dump_file = f'{resultpath}/prediction_{dataset_file_name.split(\"/\")[-1].split(\".json\")[0]}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}_result.json'\n",
        "\n",
        "    # Initialize dictionary to store previously used data (if any)\n",
        "    useddata = {}\n",
        "    if os.path.exists(normal_dump_file):\n",
        "        with open(normal_dump_file) as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data  # Store previously saved results to avoid duplicate predictions\n",
        "    else:\n",
        "        if not os.path.exists(resultpath):\n",
        "            os.makedirs(resultpath)  # Create the directory if it doesn't exist\n",
        "\n",
        "    results = []  # List to hold the final results\n",
        "    with open(normal_dump_file, 'w') as f:\n",
        "        for instance in tqdm.tqdm(instances):  # Loop through each instance in the dataset\n",
        "            # If this instance's results have already been processed, use the saved data\n",
        "            if instance['id'] in useddata and instance['query'] == useddata[instance['id']]['query'] and instance['answer'] == useddata[instance['id']]['ans']:\n",
        "                results.append(useddata[instance['id']])  # Append the existing result\n",
        "                f.write(json.dumps(useddata[instance['id']], ensure_ascii=False) + '\\n')  # Write to the result file\n",
        "                continue  # Skip further processing for this instance\n",
        "\n",
        "            try:\n",
        "                random.seed(2333)  # Ensure reproducibility by setting a fixed random seed\n",
        "                # If passage_num is 0, there are no documents to use for context\n",
        "                if passage_num == 0:\n",
        "                    query = instance['query']\n",
        "                    ans = instance['answer']\n",
        "                    docs = []  # No documents for the query\n",
        "                else:\n",
        "                    # Process the data based on noise rate and passage number\n",
        "                    query, ans, docs = processdata(instance, noise_rate, passage_num, dataset_file_name, correct_rate)\n",
        "\n",
        "                # Get the model's prediction for the query and documents\n",
        "                label, prediction, factlabel = predict(query, ans, docs, model, system, instruction, temperature, dataset_file_name)\n",
        "\n",
        "                # Create a new instance with the prediction and other details\n",
        "                newinstance = {\n",
        "                    'id': instance['id'],\n",
        "                    'query': query,\n",
        "                    'ans': ans,\n",
        "                    'label': label,\n",
        "                    'prediction': prediction,\n",
        "                    'docs': docs,\n",
        "                    'noise_rate': noise_rate,\n",
        "                    'factlabel': factlabel\n",
        "                }\n",
        "\n",
        "                # Append the result to the results list and save it to the file\n",
        "                results.append(newinstance)\n",
        "                f.write(json.dumps(newinstance, ensure_ascii=False) + '\\n')\n",
        "            except Exception as e:\n",
        "                print(\"Error:\", e)  # Handle any exceptions that may occur during prediction\n",
        "                continue  # Skip to the next instance if there's an error\n",
        "\n",
        "    tt = 0  # Initialize counter for the number of correct predictions\n",
        "    for i in results:\n",
        "        label = i['label']\n",
        "        # Count the correct predictions based on noise rate and labels\n",
        "        if noise_rate == 1 and label[0] == -1:\n",
        "            tt += 1\n",
        "        elif 0 not in label and 1 in label:\n",
        "            tt += 1\n",
        "\n",
        "    # Calculate the accuracy and other statistics\n",
        "    scores = {\n",
        "        'all_rate': (tt) / len(results),  # Overall accuracy\n",
        "        'noise_rate': noise_rate,\n",
        "        'tt': tt,\n",
        "        'nums': len(results)\n",
        "    }\n",
        "\n",
        "    # If the dataset is fact-checking, calculate the fact-checking statistics\n",
        "    if '_fact' in dataset_file_name:\n",
        "        fact_tt = 0  # Initialize counter for fact-checking instances\n",
        "        correct_tt = 0  # Initialize counter for correct fact-checking predictions\n",
        "        for i in results:\n",
        "            if i['factlabel'] == 1:  # If there's a factual error\n",
        "                fact_tt += 1\n",
        "                if 0 not in i['label']:  # If the prediction was correct\n",
        "                    correct_tt += 1\n",
        "        fact_check_rate = fact_tt / len(results)  # Rate of factual error instances\n",
        "        if fact_tt > 0:\n",
        "            correct_rate = correct_tt / fact_tt  # Accuracy for fact-checking instances\n",
        "        else:\n",
        "            correct_rate = 0\n",
        "        scores['fact_check_rate'] = fact_check_rate\n",
        "        scores['correct_rate'] = correct_rate\n",
        "        scores['fact_tt'] = fact_tt\n",
        "        scores['correct_tt'] = correct_tt\n",
        "\n",
        "    # Save the calculated scores to a result file\n",
        "    json.dump(scores, open(result_dump_file, 'w'), ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Calculate and return the overall accuracy\n",
        "    accuracy = (tt / len(results))\n",
        "    return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "fiRCqPFGnEHQ",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(Metric, model_name, temperature, noise_rate, correct_rate, passage_num, l_Number_of_questions):\n",
        "    # Set a global variable for the number of questions\n",
        "    global Number_of_questions\n",
        "    Number_of_questions = int(l_Number_of_questions)\n",
        "\n",
        "    # Checking which metric to use (Noise Robustness, Negative Rejection, Information Integration, or Counterfactual Robustness)\n",
        "    if Metric == \"Noise Robustness\":\n",
        "        # Evaluate the model using the 'Noise Robustness' metric\n",
        "        # The evalue function is used to evaluate the model with the given temperature, noise rate, correct rate, passage number, etc.\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num), False)\n",
        "        # Store the accuracy result for the output\n",
        "        output_text = \"accuracy = \" + str(value)\n",
        "\n",
        "    elif Metric == \"Negative Rejection\":\n",
        "        # Evaluate the model using the 'Negative Rejection' metric (this is often used to evaluate how well a system rejects irrelevant data)\n",
        "        # Noise rate is fixed to 1.0 here because we are not evaluating noise robustness but rather rejection ability.\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en.json\", float(temperature), 1.0, float(correct_rate), int(passage_num), False)\n",
        "        # Uncomment the following line if the reject_evalue function was used for rejection evaluation\n",
        "        # value = reject_evalue(model_name, RGB_Config_folder_path + \"en.json\", float(temperature), 1.0, float(correct_rate), int(passage_num))\n",
        "        # Set the rejection rate as the output\n",
        "        output_text = \"rejection rate = \" + str(value)\n",
        "\n",
        "    elif Metric == \"Information Integration\":\n",
        "        # Evaluate the model using the 'Information Integration' metric\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en_int.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num), False)\n",
        "        # Store the accuracy result for information integration\n",
        "        output_text = \"accuracy = \" + str(value)\n",
        "\n",
        "    elif Metric == \"Counterfactual Robustness\":\n",
        "        # Evaluate the model using the 'Counterfactual Robustness' metric (tests how robust the model is to counterfactual changes)\n",
        "        # The evalue function evaluates the accuracy, and fact_evalue evaluates the ability to detect and correct factual errors\n",
        "        value = evalue(model_name, RGB_Config_folder_path + \"en_fact.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num), True)\n",
        "        # Get the error detection and correction rates using the fact_evalue function\n",
        "        all_rate, reject_rate, correct_rate = fact_evalue(model_name, RGB_Config_folder_path + \"en_fact.json\", float(temperature), float(noise_rate), float(correct_rate), int(passage_num))\n",
        "        # Combine the accuracy with the error detection and correction rates in the output\n",
        "        output_text = \"accuracy = \" + str(value) + \"\\nerror detection rate = \" + str(reject_rate) + \"\\nerror correction rate = \" + str(correct_rate)\n",
        "\n",
        "    else:\n",
        "        # If an invalid metric is provided, exit the program\n",
        "        print(\"Invalid Metric\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    return output_text  # Return the resulting output text\n"
      ],
      "metadata": {
        "id": "pzN9eef6gTz7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Gradio Blocks interface for the model evaluation\n",
        "with gr.Blocks() as iface:\n",
        "\n",
        "    # Display a markdown title at the top of the interface\n",
        "    gr.Markdown(\"# Run your model with different parameters\")\n",
        "\n",
        "    # Create a row for placing multiple columns side by side\n",
        "    with gr.Row():\n",
        "\n",
        "        # First column for the first set of input fields\n",
        "        with gr.Column():\n",
        "\n",
        "            # Dropdown for selecting the evaluation metric\n",
        "            metric = gr.Dropdown(\n",
        "                choices=[\"Noise Robustness\", \"Negative Rejection\", \"Information Integration\", \"Counterfactual Robustness\"],\n",
        "                label=\"Metric\"  # Label for this dropdown\n",
        "            )\n",
        "\n",
        "            # Dropdown for selecting the model name from a list of models\n",
        "            model_name = gr.Dropdown(\n",
        "                choices=[\"gpt-3.5-turbo\", \"qwen-2.5-32b\", \"DeepSeek-R1-Distill-Llama-70B\", \"Mixtral-8x7B-Instruct\", \"llama3-70b-8192\", \"gemma-2-9b-it\"],\n",
        "                label=\"Model Name\"  # Label for this dropdown\n",
        "            )\n",
        "\n",
        "            # Slider for selecting the temperature value (0 to 1)\n",
        "            temperature = gr.Slider(\n",
        "                value=0.2,  # Initial value set to 0.2\n",
        "                minimum=0.0,  # Minimum value is 0.0\n",
        "                maximum=1.0,  # Maximum value is 1.0\n",
        "                label=\"Temperature (0 - 1)\"  # Label for the slider\n",
        "            )\n",
        "\n",
        "        # Second column for the second set of input fields\n",
        "        with gr.Column():\n",
        "\n",
        "            # Slider for selecting the noise rate (0 to 1)\n",
        "            noise_rate = gr.Slider(\n",
        "                value=0.2,  # Initial value set to 0.2\n",
        "                minimum=0.0,  # Minimum value is 0.0\n",
        "                maximum=1.0,  # Maximum value is 1.0\n",
        "                label=\"Noise Rate (0 - 1)\"  # Label for the slider\n",
        "            )\n",
        "\n",
        "            # Slider for selecting the correct rate (0 to 1)\n",
        "            correct_rate = gr.Slider(\n",
        "                value=0.2,  # Initial value set to 0.2\n",
        "                minimum=0.0,  # Minimum value is 0.0\n",
        "                maximum=1.0,  # Maximum value is 1.0\n",
        "                label=\"Correct Rate (0 - 1)\"  # Label for the slider\n",
        "            )\n",
        "\n",
        "            # Number input for selecting the number of passages (1 to 10)\n",
        "            passage_number = gr.Number(\n",
        "                value=5,  # Initial value set to 5\n",
        "                minimum=1,  # Minimum value is 1\n",
        "                maximum=10,  # Maximum value is 10\n",
        "                precision=0,  # No decimal points\n",
        "                label=\"Passage Number (1-10)\"  # Label for the number input\n",
        "            )\n",
        "\n",
        "            # Number input for selecting the number of questions (1 to 300)\n",
        "            l_Number_of_questions = gr.Number(\n",
        "                value=10,  # Initial value set to 10\n",
        "                minimum=1,  # Minimum value is 1\n",
        "                maximum=300,  # Maximum value is 300\n",
        "                precision=0,  # No decimal points\n",
        "                label=\"Number of questions (1-300)\"  # Label for the number input\n",
        "            )\n",
        "\n",
        "    # Output textbox where the results of the model evaluation will be displayed\n",
        "    output = gr.Textbox(label=\"Output\", lines=3)\n",
        "\n",
        "    # Button to trigger the model evaluation when clicked\n",
        "    submit_button = gr.Button(\"Run Model\")\n",
        "\n",
        "    # Define the action when the button is clicked\n",
        "    submit_button.click(\n",
        "        run_model,  # The function to be called when the button is clicked\n",
        "        [metric, model_name, temperature, noise_rate, correct_rate, passage_number, l_Number_of_questions],  # Inputs to be passed to the function\n",
        "        output  # Output where the results will be displayed\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface with error display and debugging enabled\n",
        "iface.launch(show_error=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AkynZkqogQVK",
        "outputId": "9831cf35-278c-4ffa-928a-b9c0b4159a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://742feb0e8bacb0a56d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://742feb0e8bacb0a56d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:51<00:00, 11.14s/it]\n",
            "100%|██████████| 10/10 [02:05<00:00, 12.51s/it]\n",
            "100%|██████████| 10/10 [02:26<00:00, 14.63s/it]\n",
            "100%|██████████| 10/10 [01:32<00:00,  9.28s/it]\n",
            " 90%|█████████ | 9/10 [00:01<00:00, 12.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 499016, Requested 1640. Please try again in 1m53.3138s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 499016, Requested 1740. Please try again in 2m10.5548s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 499016, Requested 1894. Please try again in 2m37.126s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 499016, Requested 1674. Please try again in 1m59.071999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 499015, Requested 1574. Please try again in 1m41.753s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 499015, Requested 1574. Please try again in 1m41.716s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 10/10 [00:01<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 499015, Requested 1830. Please try again in 2m25.9098s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [00:00<00:00, 42.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498899, Requested 1640. Please try again in 1m33.1248s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498899, Requested 1740. Please try again in 1m50.3648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498899, Requested 1894. Please try again in 2m16.934s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498899, Requested 1674. Please try again in 1m38.874999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498898, Requested 1574. Please try again in 1m21.553s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 30.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498898, Requested 1574. Please try again in 1m21.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498898, Requested 1830. Please try again in 2m5.7108s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|█████     | 5/10 [00:00<00:00, 36.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498825, Requested 1640. Please try again in 1m20.196799999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498824, Requested 1740. Please try again in 1m37.4288s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498824, Requested 1894. Please try again in 2m3.999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498824, Requested 1674. Please try again in 1m25.943999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498824, Requested 1574. Please try again in 1m8.623s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 28.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498823, Requested 1574. Please try again in 1m8.577s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
            "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01jfm75kxeenzbp5hhvgzxy7nz` service tier `on_demand` on : Limit 500000, Used 498823, Requested 1830. Please try again in 1m52.7708s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:02<00:19,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:03<00:14,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:05<00:12,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:07<00:10,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:18<00:25,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:32<00:32,  8.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:47<00:31, 10.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:03<00:24, 12.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:16<00:12, 12.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:32<00:00,  9.21s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:01<00:12,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:03<00:13,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:04<00:11,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:06<00:09,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:09<00:10,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:18<00:17,  4.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:33<00:23,  7.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:50<00:21, 10.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:03<00:11, 11.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:17<00:00,  7.74s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:01<00:11,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:03<00:14,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:04<00:11,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:06<00:09,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:12<00:16,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:24<00:24,  6.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:39<00:27,  9.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:55<00:22, 11.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:09<00:12, 12.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:24<00:00,  8.42s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:01<00:12,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:02<00:11,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:04<00:10,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:06<00:10,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:17<00:24,  4.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:29<00:29,  7.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:44<00:29,  9.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:01<00:24, 12.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:14<00:12, 12.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}